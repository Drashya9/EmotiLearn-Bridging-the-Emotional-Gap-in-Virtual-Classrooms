{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Go9LjtiG9_Ps",
        "outputId": "cb117a46-31f0-444f-b005-a5f2e017f02c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: torch==2.6.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.6.0+cu124)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (4.13.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch==2.6.0->torchvision)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch==2.6.0->torchvision) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch==2.6.0->torchvision) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch==2.6.0->torchvision) (3.0.2)\n",
            "Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m92.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m40.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
            "ğŸ“ Device: cpu\n"
          ]
        }
      ],
      "source": [
        "!pip install torchvision\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision import transforms, models\n",
        "from PIL import Image\n",
        "import pandas as pd\n",
        "from torch.cuda.amp import autocast, GradScaler\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"ğŸ“ Device:\", device)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ckdQiph0-brf",
        "outputId": "ae0dbeca-fcff-4cd8-9d28-c8dd8c3ca08e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the full label file and create a subset CSV\n",
        "csv_path_full = \"/content/drive/MyDrive/Data Mining Project/AllLabels.csv\"\n",
        "df = pd.read_csv(csv_path_full)\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "# Sample 200 clips for faster training\n",
        "sampled_df = df.sample(n=200, random_state=42)\n",
        "small_csv_path = \"/content/drive/MyDrive/Data Mining Project/AllLabels.csv_200.csv\"\n",
        "sampled_df.to_csv(small_csv_path, index=False)\n"
      ],
      "metadata": {
        "id": "9BQ6JXUS-LbN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DAiSEESequenceDataset(Dataset):\n",
        "    def __init__(self, csv_path, image_root, transform, max_frames=5):\n",
        "        self.data = pd.read_csv(csv_path)\n",
        "        self.data.columns = self.data.columns.str.strip()\n",
        "        self.image_root = image_root\n",
        "        self.transform = transform\n",
        "        self.max_frames = max_frames\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.data.iloc[idx]\n",
        "        clip_id = row['ClipID'].replace(\".avi\", \"\")\n",
        "        group_id = clip_id[:6]\n",
        "        folder_path = os.path.join(self.image_root, group_id, clip_id)\n",
        "\n",
        "        images = []\n",
        "        for i in range(self.max_frames):\n",
        "            img_path = os.path.join(folder_path, f\"image_{i:03d}.jpg\")\n",
        "            if os.path.exists(img_path):\n",
        "                img = Image.open(img_path).convert(\"RGB\")\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                images.append(img)\n",
        "\n",
        "        if len(images) == 0:\n",
        "            dummy = torch.zeros((self.max_frames, 3, 112, 112))\n",
        "            labels = torch.tensor([\n",
        "                row['Boredom'],\n",
        "                row['Engagement'],\n",
        "                row['Confusion'],\n",
        "                row['Frustration']\n",
        "            ], dtype=torch.long)\n",
        "            return dummy, labels\n",
        "\n",
        "        while len(images) < self.max_frames:\n",
        "            images.append(torch.zeros_like(images[0]))\n",
        "\n",
        "        images = torch.stack(images)\n",
        "        labels = torch.tensor([\n",
        "            row['Boredom'],\n",
        "            row['Engagement'],\n",
        "            row['Confusion'],\n",
        "            row['Frustration']\n",
        "        ], dtype=torch.long)\n",
        "\n",
        "        return images, labels\n"
      ],
      "metadata": {
        "id": "tnvD9dpY-jkD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CNN_GRU_MultiHead(nn.Module):\n",
        "    def __init__(self, hidden_dim=128, num_layers=1):\n",
        "        super().__init__()\n",
        "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "        modules = list(resnet.children())[:-1]\n",
        "        self.cnn = nn.Sequential(*modules)\n",
        "        self.gru = nn.GRU(512, hidden_dim, num_layers, batch_first=True)\n",
        "        self.heads = nn.ModuleList([nn.Linear(hidden_dim, 4) for _ in range(4)])\n",
        "\n",
        "    def forward(self, x_seq):  # [B, T, C, H, W]\n",
        "        B, T, C, H, W = x_seq.shape\n",
        "        x_seq = x_seq.view(B * T, C, H, W)\n",
        "        feats = self.cnn(x_seq).view(B, T, -1)  # [B, T, 512]\n",
        "        gru_out, _ = self.gru(feats)\n",
        "        last_hidden = gru_out[:, -1, :]\n",
        "        return [head(last_hidden) for head in self.heads]\n"
      ],
      "metadata": {
        "id": "EgtkrWcv-msz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((112, 112)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406],\n",
        "                         [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "csv_path = \"/content/drive/MyDrive/Data Mining Project/AllLabels.csv_200.csv\"\n",
        "image_root = \"/content/drive/MyDrive/Data Mining Project/DAiSEE_Frames_Every3s/Train\"\n",
        "\n",
        "dataset = DAiSEESequenceDataset(csv_path, image_root, transform, max_frames=5)\n",
        "loader = DataLoader(dataset, batch_size=32, shuffle=True)\n"
      ],
      "metadata": {
        "id": "uhoOZ87N-pBy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = CNN_GRU_MultiHead().to(device)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "scaler = GradScaler()\n",
        "\n",
        "epochs = 5\n",
        "\n",
        "for epoch in range(1, epochs + 1):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = [0, 0, 0, 0]\n",
        "    total = [0, 0, 0, 0]\n",
        "\n",
        "    print(f\"\\nğŸ“˜ Epoch {epoch}/{epochs} ------------------------\")\n",
        "\n",
        "    for batch_idx, (x_seq, labels) in enumerate(loader):\n",
        "        x_seq, labels = x_seq.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        with autocast():\n",
        "            outputs = model(x_seq)\n",
        "            loss = sum([criterion(out, labels[:, i]) for i, out in enumerate(outputs)])\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "        scaler.step(optimizer)\n",
        "        scaler.update()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        for i in range(4):\n",
        "            preds = outputs[i].argmax(dim=1)\n",
        "            correct[i] += (preds == labels[:, i]).sum().item()\n",
        "            total[i] += labels.size(0)\n",
        "\n",
        "        if (batch_idx + 1) % 2 == 0 or (batch_idx + 1) == len(loader):\n",
        "            print(f\"Batch {batch_idx + 1}/{len(loader)} | Batch Loss: {loss.item():.4f}\")\n",
        "\n",
        "    print(f\"\\nâœ… Epoch {epoch} Summary:\")\n",
        "    print(f\"Avg Loss: {total_loss / len(loader):.4f}\")\n",
        "    for i, name in enumerate(['Boredom', 'Engagement', 'Confusion', 'Frustration']):\n",
        "        acc = 100 * correct[i] / total[i]\n",
        "        print(f\"{name} Accuracy: {acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4OQ5OJwf-2rm",
        "outputId": "1bf526b4-aec5-476e-a138-91bafb02b5cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 44.7M/44.7M [00:00<00:00, 56.5MB/s]\n",
            "<ipython-input-8-9e3908a9eba6>:4: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = GradScaler()\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/grad_scaler.py:132: UserWarning: torch.cuda.amp.GradScaler is enabled, but CUDA is not available.  Disabling.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ“˜ Epoch 1/5 ------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-9e3908a9eba6>:21: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with autocast():\n",
            "/usr/local/lib/python3.11/dist-packages/torch/amp/autocast_mode.py:266: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch 2/7 | Batch Loss: 5.4002\n",
            "Batch 4/7 | Batch Loss: 4.4218\n",
            "Batch 6/7 | Batch Loss: 4.3293\n",
            "Batch 7/7 | Batch Loss: 4.0913\n",
            "\n",
            "âœ… Epoch 1 Summary:\n",
            "Avg Loss: 4.7635\n",
            "Boredom Accuracy: 44.50%\n",
            "Engagement Accuracy: 38.50%\n",
            "Confusion Accuracy: 48.00%\n",
            "Frustration Accuracy: 58.50%\n",
            "\n",
            "ğŸ“˜ Epoch 2/5 ------------------------\n",
            "Batch 2/7 | Batch Loss: 3.7163\n",
            "Batch 4/7 | Batch Loss: 3.8250\n",
            "Batch 6/7 | Batch Loss: 3.4883\n",
            "Batch 7/7 | Batch Loss: 3.0959\n",
            "\n",
            "âœ… Epoch 2 Summary:\n",
            "Avg Loss: 3.7367\n",
            "Boredom Accuracy: 46.00%\n",
            "Engagement Accuracy: 55.00%\n",
            "Confusion Accuracy: 67.50%\n",
            "Frustration Accuracy: 79.00%\n",
            "\n",
            "ğŸ“˜ Epoch 3/5 ------------------------\n",
            "Batch 2/7 | Batch Loss: 3.5377\n",
            "Batch 4/7 | Batch Loss: 4.0177\n",
            "Batch 6/7 | Batch Loss: 3.3948\n",
            "Batch 7/7 | Batch Loss: 3.0461\n",
            "\n",
            "âœ… Epoch 3 Summary:\n",
            "Avg Loss: 3.5051\n",
            "Boredom Accuracy: 47.50%\n",
            "Engagement Accuracy: 62.00%\n",
            "Confusion Accuracy: 67.50%\n",
            "Frustration Accuracy: 79.00%\n",
            "\n",
            "ğŸ“˜ Epoch 4/5 ------------------------\n",
            "Batch 2/7 | Batch Loss: 3.3410\n",
            "Batch 4/7 | Batch Loss: 3.3565\n",
            "Batch 6/7 | Batch Loss: 3.3509\n",
            "Batch 7/7 | Batch Loss: 4.1731\n",
            "\n",
            "âœ… Epoch 4 Summary:\n",
            "Avg Loss: 3.5071\n",
            "Boredom Accuracy: 51.50%\n",
            "Engagement Accuracy: 62.50%\n",
            "Confusion Accuracy: 68.00%\n",
            "Frustration Accuracy: 79.00%\n",
            "\n",
            "ğŸ“˜ Epoch 5/5 ------------------------\n",
            "Batch 2/7 | Batch Loss: 3.0595\n",
            "Batch 4/7 | Batch Loss: 3.6265\n",
            "Batch 6/7 | Batch Loss: 3.2976\n",
            "Batch 7/7 | Batch Loss: 4.0156\n",
            "\n",
            "âœ… Epoch 5 Summary:\n",
            "Avg Loss: 3.3787\n",
            "Boredom Accuracy: 51.50%\n",
            "Engagement Accuracy: 64.00%\n",
            "Confusion Accuracy: 68.00%\n",
            "Frustration Accuracy: 79.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DAiSEEValidationDataset(Dataset):\n",
        "    def __init__(self, label_csv, image_root, transform=None, max_frames=5):\n",
        "        self.df = pd.read_csv(label_csv)\n",
        "        self.df.columns = self.df.columns.str.strip()\n",
        "        self.image_root = image_root\n",
        "        self.transform = transform\n",
        "        self.max_frames = max_frames\n",
        "        self.valid_clips = []\n",
        "\n",
        "        for _, row in self.df.iterrows():\n",
        "            clip_id = row[\"ClipID\"].replace(\".avi\", \"\")\n",
        "            group_id = clip_id[:6]\n",
        "            folder_path = os.path.join(image_root, group_id, clip_id)\n",
        "            if os.path.exists(os.path.join(folder_path, \"image_000.jpg\")):\n",
        "                self.valid_clips.append((folder_path, row))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_clips)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        folder_path, row = self.valid_clips[idx]\n",
        "        images = []\n",
        "\n",
        "        for i in range(self.max_frames):\n",
        "            img_path = os.path.join(folder_path, f\"image_{i:03d}.jpg\")\n",
        "            if os.path.exists(img_path):\n",
        "                img = Image.open(img_path).convert(\"RGB\")\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                images.append(img)\n",
        "\n",
        "        if len(images) == 0:\n",
        "            images = [torch.zeros((3, 112, 112)) for _ in range(self.max_frames)]\n",
        "\n",
        "        while len(images) < self.max_frames:\n",
        "            images.append(torch.zeros_like(images[0]))\n",
        "\n",
        "        images = torch.stack(images)\n",
        "\n",
        "        labels = torch.tensor([\n",
        "            row['Boredom'],\n",
        "            row['Engagement'],\n",
        "            row['Confusion'],\n",
        "            row['Frustration']\n",
        "        ], dtype=torch.long)\n",
        "\n",
        "        return images, labels\n"
      ],
      "metadata": {
        "id": "qBO7KsbNYWhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "val_csv = \"/content/drive/MyDrive/Data Mining Project/AllLabels.csv_200.csv\"\n",
        "val_root = \"/content/drive/MyDrive/Data Mining Project/DAiSEE_Frames_Every3s/Validation\"\n",
        "\n",
        "val_dataset = DAiSEEValidationDataset(val_csv, val_root, transform, max_frames=5)\n",
        "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"âœ… Loaded {len(val_dataset)} validation samples.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IM3ukhhqZJAk",
        "outputId": "bdd12dba-da0e-4bf7-adef-f610d0925e54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded 29 validation samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = [0, 0, 0, 0]\n",
        "total = [0, 0, 0, 0]\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in val_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        outputs = model(imgs)\n",
        "\n",
        "        for i in range(4):\n",
        "            preds = outputs[i].argmax(dim=1)\n",
        "            correct[i] += (preds == labels[:, i]).sum().item()\n",
        "            total[i] += labels.size(0)\n",
        "\n",
        "print(\"\\nğŸ¯ Final Validation Accuracy:\")\n",
        "for i, name in enumerate(['Boredom', 'Engagement', 'Confusion', 'Frustration']):\n",
        "    acc = 100 * correct[i] / total[i]\n",
        "    print(f\"{name}: {acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B3c8RyXzZUm2",
        "outputId": "29dcc5cb-64a8-4139-d01f-94352cc2a1db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ¯ Final Validation Accuracy:\n",
            "Boredom: 24.14%\n",
            "Engagement: 34.48%\n",
            "Confusion: 72.41%\n",
            "Frustration: 72.41%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DAiSEEValidationDataset(Dataset):  # Works for both val and test\n",
        "    def __init__(self, label_csv, image_root, transform=None, max_frames=5):\n",
        "        self.df = pd.read_csv(label_csv)\n",
        "        self.df.columns = self.df.columns.str.strip()\n",
        "        self.image_root = image_root\n",
        "        self.transform = transform\n",
        "        self.max_frames = max_frames\n",
        "        self.valid_clips = []\n",
        "\n",
        "        for _, row in self.df.iterrows():\n",
        "            clip_id = row[\"ClipID\"].replace(\".avi\", \"\")\n",
        "            group_id = clip_id[:6]\n",
        "            folder_path = os.path.join(image_root, group_id, clip_id)\n",
        "            if os.path.exists(os.path.join(folder_path, \"image_000.jpg\")):\n",
        "                self.valid_clips.append((folder_path, row))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.valid_clips)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        folder_path, row = self.valid_clips[idx]\n",
        "        images = []\n",
        "\n",
        "        for i in range(self.max_frames):\n",
        "            img_path = os.path.join(folder_path, f\"image_{i:03d}.jpg\")\n",
        "            if os.path.exists(img_path):\n",
        "                img = Image.open(img_path).convert(\"RGB\")\n",
        "                if self.transform:\n",
        "                    img = self.transform(img)\n",
        "                images.append(img)\n",
        "\n",
        "        if len(images) == 0:\n",
        "            images = [torch.zeros((3, 112, 112)) for _ in range(self.max_frames)]\n",
        "\n",
        "        while len(images) < self.max_frames:\n",
        "            images.append(torch.zeros_like(images[0]))\n",
        "\n",
        "        images = torch.stack(images)\n",
        "\n",
        "        labels = torch.tensor([\n",
        "            row['Boredom'],\n",
        "            row['Engagement'],\n",
        "            row['Confusion'],\n",
        "            row['Frustration']\n",
        "        ], dtype=torch.long)\n",
        "\n",
        "        return images, labels\n"
      ],
      "metadata": {
        "id": "DE2KJ0LKZXmY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_csv = \"/content/drive/MyDrive/Data Mining Project/AllLabels.csv_200.csv\"\n",
        "test_root = \"/content/drive/MyDrive/Data Mining Project/DAiSEE_Frames_Every3s/Test\"\n",
        "\n",
        "test_dataset = DAiSEEValidationDataset(test_csv, test_root, transform, max_frames=5)\n",
        "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
        "\n",
        "print(f\"âœ… Loaded {len(test_dataset)} test samples.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dpbNzckfbgmJ",
        "outputId": "a934adeb-441e-4b42-9ae2-eab1a98ba5aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Loaded 49 test samples.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "correct = [0, 0, 0, 0]\n",
        "total = [0, 0, 0, 0]\n",
        "\n",
        "with torch.no_grad():\n",
        "    for imgs, labels in test_loader:\n",
        "        imgs, labels = imgs.to(device), labels.to(device)\n",
        "        outputs = model(imgs)\n",
        "\n",
        "        for i in range(4):\n",
        "            preds = outputs[i].argmax(dim=1)\n",
        "            correct[i] += (preds == labels[:, i]).sum().item()\n",
        "            total[i] += labels.size(0)\n",
        "\n",
        "print(\"\\nğŸ§ª Final Test Accuracy:\")\n",
        "for i, name in enumerate(['Boredom', 'Engagement', 'Confusion', 'Frustration']):\n",
        "    acc = 100 * correct[i] / total[i]\n",
        "    print(f\"{name}: {acc:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "acTU_Zbebr3a",
        "outputId": "a36cad14-3e32-47ef-ba17-121e4ce3d73c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "ğŸ§ª Final Test Accuracy:\n",
            "Boredom: 38.78%\n",
            "Engagement: 51.02%\n",
            "Confusion: 73.47%\n",
            "Frustration: 83.67%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "k36wJ8GybtuZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}